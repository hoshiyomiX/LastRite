name: Performance Benchmark Monitor

on:
  schedule:
    # Run every 15 minutes
    - cron: '*/15 * * * *'
  
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '60'
      concurrency:
        description: 'Concurrent requests'
        required: false
        default: '50'
  
  pull_request:
    branches:
      - main
    paths:
      - '_worker.js'
      - '.github/workflows/benchmark-monitor.yml'

env:
  WORKER_URL: ${{ secrets.WORKER_URL || 'https://lastrite.workers.dev' }}
  BENCHMARK_DURATION: 60
  CONCURRENT_REQUESTS: 50
  
jobs:
  benchmark:
    name: Run Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: testing
          fetch-depth: 0
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install dependencies
        run: |
          npm install -g autocannon
          npm install node-fetch@2
      
      - name: Run benchmark tests
        id: benchmark
        run: |
          echo "Starting benchmark at $(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          
          # Create metrics directory
          mkdir -p .github/monitoring/metrics
          
          # Run autocannon benchmark
          autocannon \
            --connections ${{ env.CONCURRENT_REQUESTS }} \
            --duration ${{ github.event.inputs.duration || env.BENCHMARK_DURATION }} \
            --json \
            "${{ env.WORKER_URL }}/api/v1/sub?cc=US&limit=20" \
            > benchmark-raw.json
          
          # Extract key metrics
          cat benchmark-raw.json | jq '{
            timestamp: now | todate,
            duration: .duration,
            requests: .requests,
            throughput: .throughput,
            latency: {
              mean: .latency.mean,
              p50: .latency.p50,
              p75: .latency.p75,
              p90: .latency.p90,
              p95: .latency.p95,
              p99: .latency.p99,
              max: .latency.max
            },
            requests_per_sec: .requests.average,
            bytes_per_sec: .throughput.average,
            errors: .errors,
            timeouts: .timeouts,
            non2xx: .non2xx
          }' > metrics-summary.json
          
          echo "Benchmark completed"
          cat metrics-summary.json
      
      - name: Test deduplication effectiveness
        id: dedup_test
        run: |
          node << 'EOF'
          const fetch = require('node-fetch');
          
          async function testDeduplication() {
            const url = '${{ env.WORKER_URL }}/api/v1/sub?cc=US&limit=20';
            const concurrency = 100;
            
            console.log(`Testing deduplication with ${concurrency} concurrent identical requests...`);
            
            const start = Date.now();
            
            // Send 100 identical requests simultaneously
            const promises = Array(concurrency).fill(null).map(() => 
              fetch(url).then(r => ({
                status: r.status,
                dedupStats: r.headers.get('x-dedup-stats'),
                cacheStatus: r.headers.get('x-cache-status')
              }))
            );
            
            const results = await Promise.all(promises);
            const duration = Date.now() - start;
            
            // Parse dedup stats from last response
            const lastStats = results[results.length - 1].dedupStats;
            const statsMatch = lastStats?.match(/hits=(\d+) misses=(\d+) saved=(\d+)/);
            
            const metrics = {
              timestamp: new Date().toISOString(),
              test: 'deduplication',
              concurrency,
              duration_ms: duration,
              avg_latency_ms: duration / concurrency,
              dedup_hits: statsMatch ? parseInt(statsMatch[1]) : 0,
              dedup_misses: statsMatch ? parseInt(statsMatch[2]) : 0,
              dedup_saved: statsMatch ? parseInt(statsMatch[3]) : 0,
              dedup_efficiency: statsMatch ? (parseInt(statsMatch[1]) / (parseInt(statsMatch[1]) + parseInt(statsMatch[2])) * 100).toFixed(2) : 0,
              cache_hits: results.filter(r => r.cacheStatus === 'HIT').length,
              cache_misses: results.filter(r => r.cacheStatus === 'MISS').length,
              success_rate: (results.filter(r => r.status === 200).length / results.length * 100).toFixed(2)
            };
            
            console.log(JSON.stringify(metrics, null, 2));
            require('fs').writeFileSync('dedup-metrics.json', JSON.stringify(metrics, null, 2));
          }
          
          testDeduplication().catch(console.error);
          EOF
      
      - name: Test various endpoints
        id: endpoint_test
        run: |
          node << 'EOF'
          const fetch = require('node-fetch');
          
          async function testEndpoints() {
            const baseUrl = '${{ env.WORKER_URL }}';
            const endpoints = [
              { name: 'sub_us', url: '/api/v1/sub?cc=US&limit=20' },
              { name: 'sub_sg', url: '/api/v1/sub?cc=SG&limit=20' },
              { name: 'sub_v2ray', url: '/api/v1/sub?cc=US&limit=10&format=v2ray' },
              { name: 'myip', url: '/api/v1/myip' },
            ];
            
            const results = [];
            
            for (const endpoint of endpoints) {
              const start = Date.now();
              try {
                const response = await fetch(baseUrl + endpoint.url);
                const duration = Date.now() - start;
                
                results.push({
                  name: endpoint.name,
                  url: endpoint.url,
                  status: response.status,
                  duration_ms: duration,
                  cache_status: response.headers.get('x-cache-status'),
                  dedup_stats: response.headers.get('x-dedup-stats'),
                  success: response.ok
                });
              } catch (error) {
                results.push({
                  name: endpoint.name,
                  url: endpoint.url,
                  error: error.message,
                  success: false
                });
              }
            }
            
            const metrics = {
              timestamp: new Date().toISOString(),
              endpoints: results,
              summary: {
                total: results.length,
                success: results.filter(r => r.success).length,
                failed: results.filter(r => !r.success).length,
                avg_duration_ms: (results.reduce((sum, r) => sum + (r.duration_ms || 0), 0) / results.length).toFixed(2)
              }
            };
            
            console.log(JSON.stringify(metrics, null, 2));
            require('fs').writeFileSync('endpoint-metrics.json', JSON.stringify(metrics, null, 2));
          }
          
          testEndpoints().catch(console.error);
          EOF
      
      - name: Combine metrics
        run: |
          # Combine all metrics into single file
          jq -s '
            {
              timestamp: now | todate,
              commit: "${{ github.sha }}",
              branch: "${{ github.ref_name }}",
              benchmark: .[0],
              deduplication: .[1],
              endpoints: .[2]
            }
          ' metrics-summary.json dedup-metrics.json endpoint-metrics.json > combined-metrics.json
          
          # Create filename with timestamp
          TIMESTAMP=$(date -u +"%Y%m%d-%H%M%S")
          cp combined-metrics.json ".github/monitoring/metrics/metrics-${TIMESTAMP}.json"
          
          echo "METRICS_FILE=metrics-${TIMESTAMP}.json" >> $GITHUB_ENV
      
      - name: Analyze metrics & generate recommendations
        run: |
          node << 'EOF'
          const fs = require('fs');
          const metrics = JSON.parse(fs.readFileSync('combined-metrics.json', 'utf8'));
          
          // Analysis and recommendations
          const analysis = {
            timestamp: new Date().toISOString(),
            commit: '${{ github.sha }}',
            metrics_summary: {
              p95_latency_ms: metrics.benchmark.latency.p95,
              avg_throughput_rps: metrics.benchmark.requests_per_sec,
              dedup_efficiency: parseFloat(metrics.deduplication.dedup_efficiency),
              error_rate: ((metrics.benchmark.errors + metrics.benchmark.timeouts) / metrics.benchmark.requests.total * 100).toFixed(2),
              cache_hit_rate: ((metrics.deduplication.cache_hits / (metrics.deduplication.cache_hits + metrics.deduplication.cache_misses)) * 100).toFixed(2)
            },
            health_status: 'healthy',
            recommendations: []
          };
          
          // Generate recommendations based on metrics
          if (metrics.benchmark.latency.p95 > 1000) {
            analysis.health_status = 'warning';
            analysis.recommendations.push({
              priority: 'high',
              category: 'latency',
              issue: `P95 latency is ${metrics.benchmark.latency.p95}ms (target: <1000ms)`,
              suggestion: 'Consider increasing connection pool size or adaptive timeout thresholds'
            });
          }
          
          if (parseFloat(metrics.deduplication.dedup_efficiency) < 50) {
            analysis.recommendations.push({
              priority: 'medium',
              category: 'deduplication',
              issue: `Deduplication efficiency is ${metrics.deduplication.dedup_efficiency}% (target: >70%)`,
              suggestion: 'Increase REQUEST_COALESCE_TTL from 2000ms to 3000ms for better coalescing'
            });
          }
          
          const errorRate = parseFloat(analysis.metrics_summary.error_rate);
          if (errorRate > 1) {
            analysis.health_status = 'critical';
            analysis.recommendations.push({
              priority: 'critical',
              category: 'reliability',
              issue: `Error rate is ${errorRate}% (target: <1%)`,
              suggestion: 'Check RETRY_MAX_ATTEMPTS and RETRY_BASE_DELAY settings, consider increasing retry attempts'
            });
          }
          
          if (analysis.recommendations.length === 0) {
            analysis.recommendations.push({
              priority: 'info',
              category: 'performance',
              issue: 'All metrics within acceptable ranges',
              suggestion: 'Continue monitoring. Consider A/B testing more aggressive optimizations.'
            });
          }
          
          fs.writeFileSync('analysis-report.json', JSON.stringify(analysis, null, 2));
          console.log(JSON.stringify(analysis, null, 2));
          EOF
      
      - name: Generate markdown report
        run: |
          cat << 'EOF' > performance-report.md
          # üìä Performance Benchmark Report
          
          **Generated:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          
          ---
          
          ## üéØ Key Metrics
          
          EOF
          
          node << 'NODESCRIPT'
          const fs = require('fs');
          const metrics = JSON.parse(fs.readFileSync('combined-metrics.json', 'utf8'));
          const analysis = JSON.parse(fs.readFileSync('analysis-report.json', 'utf8'));
          
          let report = fs.readFileSync('performance-report.md', 'utf8');
          
          report += `
          | Metric | Value | Status |
          |--------|-------|--------|
          | **P50 Latency** | ${metrics.benchmark.latency.p50.toFixed(2)}ms | ${metrics.benchmark.latency.p50 < 200 ? '‚úÖ' : '‚ö†Ô∏è'} |
          | **P95 Latency** | ${metrics.benchmark.latency.p95.toFixed(2)}ms | ${metrics.benchmark.latency.p95 < 1000 ? '‚úÖ' : '‚ö†Ô∏è'} |
          | **P99 Latency** | ${metrics.benchmark.latency.p99.toFixed(2)}ms | ${metrics.benchmark.latency.p99 < 2000 ? '‚úÖ' : '‚ö†Ô∏è'} |
          | **Throughput** | ${metrics.benchmark.requests_per_sec.toFixed(2)} req/s | ‚úÖ |
          | **Error Rate** | ${analysis.metrics_summary.error_rate}% | ${parseFloat(analysis.metrics_summary.error_rate) < 1 ? '‚úÖ' : '‚ùå'} |
          | **Dedup Efficiency** | ${analysis.metrics_summary.dedup_efficiency}% | ${parseFloat(analysis.metrics_summary.dedup_efficiency) > 70 ? '‚úÖ' : '‚ö†Ô∏è'} |
          | **Cache Hit Rate** | ${analysis.metrics_summary.cache_hit_rate}% | ${parseFloat(analysis.metrics_summary.cache_hit_rate) > 50 ? '‚úÖ' : '‚ö†Ô∏è'} |
          
          ## üîÑ Deduplication Performance
          
          | Metric | Value |
          |--------|-------|
          | Concurrent Requests | ${metrics.deduplication.concurrency} |
          | Dedup Hits | ${metrics.deduplication.dedup_hits} |
          | Dedup Misses | ${metrics.deduplication.dedup_misses} |
          | Requests Saved | ${metrics.deduplication.dedup_saved} |
          | Efficiency | ${metrics.deduplication.dedup_efficiency}% |
          | Avg Latency | ${metrics.deduplication.avg_latency_ms.toFixed(2)}ms |
          
          ## üì° Endpoint Tests
          
          | Endpoint | Status | Duration | Cache | Result |
          |----------|--------|----------|-------|--------|
          `;
          
          metrics.endpoints.endpoints.forEach(ep => {
            report += `| ${ep.name} | ${ep.status || 'error'} | ${ep.duration_ms?.toFixed(2) || 'N/A'}ms | ${ep.cache_status || 'N/A'} | ${ep.success ? '‚úÖ' : '‚ùå'} |\n`;
          });
          
          report += `
          ## üí° Recommendations
          
          **Health Status:** ${analysis.health_status === 'healthy' ? '‚úÖ Healthy' : analysis.health_status === 'warning' ? '‚ö†Ô∏è Warning' : '‚ùå Critical'}
          
          `;
          
          analysis.recommendations.forEach((rec, idx) => {
            const emoji = rec.priority === 'critical' ? 'üö®' : rec.priority === 'high' ? '‚ö†Ô∏è' : rec.priority === 'medium' ? 'üí°' : '‚ÑπÔ∏è';
            report += `
          ### ${emoji} ${rec.priority.toUpperCase()}: ${rec.category}
          
          **Issue:** ${rec.issue}
          
          **Suggestion:** ${rec.suggestion}
          `;
          });
          
          report += `
          ---
          
          ## üìà Historical Comparison
          
          *Trend analysis will be available after multiple benchmark runs*
          
          ---
          
          *This report is automatically generated every 15 minutes by GitHub Actions*
          `;
          
          fs.writeFileSync('performance-report.md', report);
          NODESCRIPT
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics-${{ github.run_number }}
          path: |
            combined-metrics.json
            analysis-report.json
            performance-report.md
            benchmark-raw.json
          retention-days: 30
      
      - name: Commit metrics to repository
        if: github.event_name == 'schedule'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add .github/monitoring/metrics/
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: add performance metrics $(date -u +"%Y-%m-%d %H:%M UTC")
          
          Metrics Summary:
          - P95 Latency: $(jq -r '.benchmark.latency.p95' combined-metrics.json)ms
          - Throughput: $(jq -r '.benchmark.requests_per_sec' combined-metrics.json) req/s
          - Dedup Efficiency: $(jq -r '.deduplication.dedup_efficiency' combined-metrics.json)%
          
          [skip ci]"
            git push
          fi
      
      - name: Comment on PR (if PR event)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
      
      - name: Create issue on critical status
        if: always()
        run: |
          HEALTH_STATUS=$(jq -r '.health_status' analysis-report.json)
          
          if [ "$HEALTH_STATUS" = "critical" ]; then
            echo "Critical performance issue detected!"
            
            cat << EOF > issue-body.md
          ## üö® Critical Performance Alert
          
          **Detected at:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Commit:** ${{ github.sha }}
          
          ### Issues Detected:
          
          $(jq -r '.recommendations[] | select(.priority == "critical") | "- **\(.category)**: \(.issue)\n  - Suggestion: \(.suggestion)"' analysis-report.json)
          
          ### Full Report:
          
          [View Performance Report Artifact](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          ### Action Required:
          
          Please review the performance metrics and implement suggested optimizations.
          
          ---
          
          *This issue was automatically created by the performance monitoring system*
          EOF
            
            # Note: Would need GitHub token with issues:write permission
            echo "Would create issue with body:"
            cat issue-body.md
          fi
